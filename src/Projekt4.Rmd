# A1. Validation Set 

## a. Aufteilung in Trainings- und Testdaten
Teile die Daten zufällig in zwei Teildatensätze vom Umfang 50 (Testdaten) und 100 (Trainingsdaten)
auf. Wiederhole den Vorgang drei Mal und vergleiche die Verteilungen der 6 Variablen
im jeweiligen Trainingsdatensatz mit den Verteilungen im Testdatensatz. Führe eine lineare Regression
auf dem Trainingsdatensatz durch und bestimme für alle drei Aufteilungen den Test MSE
und vergleiche diese. Interpretiere Deine Ergebnisse.
Verwendete Funktionen: sample, predict.

```{r , echo=T, collapse=T}
library(dplyr)
library(Metrics)

# Load Data
load("../data/Donald.RData")
data <- Donald_1
N <- nrow(data)
target <- "Trump"

train_size <- 100
test_size <- 50

mse_values <- c()
train_dist <- list()
test_dist <- list()

formula <- as.formula(paste(target, " ~ .")) # alle 6 Variablen
for (i in 1:3) {
  # Zufällige Aufteilung der Daten
  train_idx <- sample(1:N, train_size)
  test_idx <- sample(setdiff(1:N, train_idx), test_size)
  
  train <- data[train_idx, ]
  test <- data[test_idx, ]
  
  # Verteilungen speichern
  train_dist[[i]] <- train
  test_dist[[i]] <- test
  
  # Lineare Regression auf dem Trainingsdatensatz
  model <- lm(formula, data = train)
  
  # Vorhersagen für den Testdatensatz
  pred_test <- predict(model, test)
  # Berechnung von MSE
  mse_values[i] <- mse(test[[target]], pred_test)
}

# MSE über alle Wiederholungen
print(cbind(mse_values))
avg_mse <- mean(mse_values)

# Durchschnitt
print(cbind(avg_mse))
```
```{r, echo=T, collapse=T, warning=F}
library(Hmisc) # Macht automatisch mehrere Histogramme für alle numerischen Spalten

# Verteilung der Trainingsdatensatz
# Erster Gang
hist.data.frame(data.frame(train_dist[[1]]))
# Zweiter Gang
hist.data.frame(data.frame(train_dist[[2]]))
# Dritter Gang
hist.data.frame(data.frame(train_dist[[3]]))

# Verteilung der Testdatensatz
# Erster Gang
hist.data.frame(data.frame(test_dist[[1]]))
# Zweiter Gang
hist.data.frame(data.frame(test_dist[[2]]))
# Dritter Gang
hist.data.frame(data.frame(test_dist[[3]]))
```

- Die Verteilungen der sechs Variablen aus den verschiedenen zufälligen Aufteilungen sehen insgesamt ähnlich aus. Es gibt nur kleine Unterschiede, die statistisch nicht ins Gewicht fallen. Die Eigenschaften der Daten bleiben also stabil, auch wenn die Aufteilung wechselt.

- Die mittleren quadratischen Fehler (MSE) der drei Trainingsläufe liegen bei 35.99, 30.67 und 35.81. Der durchschnittliche Fehler beträgt 34.16. Diese Schwankungen zeigen, dass die Modellgüte zwar leicht variiert, insgesamt aber auf einem vergleichbaren Niveau bleibt.


# A2. Leave One Out Cross Validation


## a. LOOCV

Führe ein `Leave-One-Out` Cross Validation mit Hilfe einer `for` Schleife

```{r , echo=T, collapse=T}
mse_values <- c()
for (i in 1:N) {
  # Alle Daten außer der aktuellen
  train <- data[-i, ]

  model <- lm(formula, data = train)
  pred_train <- predict(model, newdata = data[i, ])
  
  mse_values[i] <- mse(pred_train,  as.numeric(data[i, target]))
}

avg_mse <- mean(mse_values)
# MSE für Leave-One-Out-Kreuzvalidierung
print(cbind(avg_mse))
```

- Im Vergleich zum durchschnittlichen MSE aus Aufgabe 1 (34.16) liegt der LOOCV-Wert etwas höher. Das zeigt, dass das Modell bei LOOCV leicht schlechter abschneidet, was daran liegen kann, dass LOOCV empfindlich auf einzelne Ausreißer reagiert.

## b. LOOCV automatisiert aus boot-Packet

Führe `Leave-One-Out` Cross Validation automatisiert mit Hilfe der Funktionen `glm` und `cv.glm` (Paket boot) durch und vergleiche das Ergebnis mit dem Ergebnis aus a).

```{r , echo=T, collapse=T}
#install.packages("boot")  
library(boot)
res <- cv.glm(data, glmfit = glm(formula, data = data), K = N)
mse <- res$delta[1]
# MSE für automatisierte Leave-One-Out-Kreuzvalidierung
print(cbind(mse))
```

- MSE von beiden Aufgaben A2.a und A2.b sind identisch.

# A3. K-Folds Cross-Validation

Führe jeweils 10 k-fache Cross Validations mit k = 5 und k = 10 mit Hilfe der Funktionen glm und cv.glm durch. Vergleiche die Ergebnisse sowohl untereinander als auch mit den Ergebnissen aus Aufgabe 1 und Aufgabe 2.

```{r , echo=T, collapse=T}
# K=10
res <- cv.glm(data, glmfit = glm(formula, data = data), K = 10)
mse_k10 <- res$delta[1]

# MSE für 10 k-fache Kreuzvalidierungen
print(cbind(mse_k10))
```

```{r , echo=T, collapse=T}
# K=5
res <- cv.glm(data, glmfit = glm(formula, data = data), K = 5)
mse_k5 <- res$delta[1]

# MSE für 5 k-fache Kreuzvalidierungen
print(cbind(mse_k5))
```

- Die Ergebnisse aller Methoden liegen im ähnlichen Bereich (zwischen ca. 34 und 38), was zeigt, dass die Modellleistung insgesamt stabil und verlässlich ist.

- Die zufälligen Aufteilungen ergeben den niedrigsten MSE, könnten aber zu leicht optimistischen Schätzungen führen.

- LOOCV ist sehr genau, aber auch rechenintensiv und empfindlich gegenüber Ausreißern.

- 10-Fold-CV liefert ein Ergebnis, das nahe an LOOCV liegt – bei viel geringerem Rechenaufwand. Es ist deshalb eine sehr gute praktische Wahl.